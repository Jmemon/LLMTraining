
tokenizer:
  name: "google-bert/bert-base-uncased"

architecture:
  n_layers: 4
  d_model: 256
  n_heads: 8
  n_kv_heads: 4
  activation: swiglu
  rope: true
  rope_theta: 10000

optimizer:
  type: adamw
  lr: 1e-5
  betas: [0.9, 0.999]
  weight_decay: 0.01

loss:
  type: perplexity

train:
  epochs: 10
  batch_size: 1024
  max_seq_len: 1024
