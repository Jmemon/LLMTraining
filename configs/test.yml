
tokenizer:
  name: "google-bert/bert-base-uncased"

architecture:
  n_layers: 4
  d_model: 256
  n_heads: 8
  n_kv_heads: 4
  rope: true
  rope_theta: 10000

train:
  epochs: 10
  batch_size: 1024
  learning_rate: 1e-5
  max_seq_len: 1024
