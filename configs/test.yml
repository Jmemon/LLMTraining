
tokenizer:
  name: "meta-llama/Meta-Llama-3-8B"

architecture:
  n_layers: 2
  d_model: 512
  n_heads: 8
  n_kv_heads: 4
  rope: true
  rope_theta: 10000

train:
  epochs: 10
  batch_size: 1024
  learning_rate: 1e-5
  max_seq_len: 1024
